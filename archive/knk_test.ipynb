{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_solution_text(names, solution, solution_text):\n",
    "    \"\"\"\n",
    "    Verifies if the solution_text correctly describes the knight/knave status of each person.\n",
    "    \n",
    "    Args:\n",
    "        names: List of names\n",
    "        solution: List of booleans (True for knight, False for knave)\n",
    "        solution_text: String describing the solution\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating if the solution_text is correct, and any discrepancies found\n",
    "    \"\"\"\n",
    "    # Make sure we have the same number of names and solutions\n",
    "    if len(names) != len(solution):\n",
    "        return False, \"Mismatch in lengths of names and solution arrays\"\n",
    "    \n",
    "    # Clean up the solution text and split by commas and 'and'\n",
    "    text = solution_text.split(\"RESULT:\")[-1].strip().replace('.', '')\n",
    "    # Handle 'and' at the end\n",
    "    text = text.replace(' and ', ', ')\n",
    "    \n",
    "    parts = text.split(', ')\n",
    "    \n",
    "    if len(parts) != len(names):\n",
    "        return False, f\"Solution text has {len(parts)} parts but there are {len(names)} people\"\n",
    "    \n",
    "    # Check each person\n",
    "    discrepancies = []\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        # Find which name this part refers to\n",
    "        name_idx = -1\n",
    "        for j, name in enumerate(names):\n",
    "            if name in part:\n",
    "                name_idx = j\n",
    "                break\n",
    "        \n",
    "        if name_idx == -1:\n",
    "            discrepancies.append(f\"Couldn't find any name in '{part}'\")\n",
    "            continue\n",
    "            \n",
    "        # Check if the knight/knave status is correct\n",
    "        is_knight = \"knight\" in part.lower()\n",
    "        is_knave = \"knave\" in part.lower()\n",
    "        \n",
    "        if is_knight and not solution[name_idx]:\n",
    "            discrepancies.append(f\"{names[name_idx]} is described as knight but should be knave\")\n",
    "        elif is_knave and solution[name_idx]:\n",
    "            discrepancies.append(f\"{names[name_idx]} is described as knave but should be knight\")\n",
    "        elif not is_knight and not is_knave:\n",
    "            discrepancies.append(f\"Couldn't determine if {names[name_idx]} is knight or knave in '{part}'\")\n",
    "    \n",
    "    return len(discrepancies) == 0, discrepancies\n",
    "\n",
    "# use the results to update the verified and discrepancies column of the data_[key]_[key]set\n",
    "def eval_dataset(data, field='solution_text', verified_col='verified', discrepancies_col='discrepancies'):\n",
    "    \"\"\"\n",
    "    Updates the dataset with verification results.\n",
    "    \n",
    "    Args:\n",
    "        data: The dataset to update\n",
    "    \"\"\"\n",
    "    verified = []\n",
    "    discrepancies = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        names = data['names'][i]\n",
    "        solution = data['solution'][i]\n",
    "        solution_text = data[field][i]\n",
    "        \n",
    "        is_verified, discrepancy_list = verify_solution_text(names, solution, solution_text)\n",
    "        \n",
    "        verified.append(is_verified)\n",
    "        discrepancies.append(\", \".join(discrepancy_list))\n",
    "    \n",
    "    data = data.add_column(verified_col, verified)\n",
    "    data = data.add_column(discrepancies_col, discrepancies)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 19:09:25,708 - INFO - Using base model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "2025-04-03 19:09:25,709 - INFO - Loading base model without quantization...\n",
      "2025-04-03 19:09:25,868 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-03 19:09:26,462 - INFO - Applying LoRA adapters...\n"
     ]
    }
   ],
   "source": [
    "# qwen-2.5-0.5B-instruct-sft-lora-countdown-search-1k\n",
    "import sys\n",
    "sys.path.append('/cs/student/msc/ml/2024/ycheah/projects/sos/stream-of-search')\n",
    "from finetune.run_adapter_model import load_model, generate, generate_batch\n",
    "\n",
    "adapter=\"chloeli/qwen-2.5-1.5B-instruct-sft-lora-countdown-search-react-1k\"\n",
    "batch_size=32\n",
    "model, tokenizer = load_model(adapter)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def message_template(example_question):\n",
    "    return [{ \"content\": f\"{example_question}.\\nConclude with the final result in EXACTLY this format:\\n```\\nSOLUTION: YES/NO\\ \\nRESULT: final_value\\n```\\nThe final_value should be statements separated by commas. For example, 'Michael is a knight, Zoey is a knight, and Ethan is a knight.'\", \"role\": \"user\" }]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import datasets\n",
    "data_ = datasets.load_dataset(\"K-and-K/knights-and-knaves\", name=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:16, 19.16s/it]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ppl score: 2.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:18, 19.70s/it]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3ppl score: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:20, 20.25s/it]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4ppl score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "context_len = 512\n",
    "temperature = 0.7\n",
    "\n",
    "keys = [\"2ppl\", \"3ppl\", \"4ppl\"]\n",
    "results = {}\n",
    "results['trajectories'] = {}\n",
    "results['scores'] = {}\n",
    "\n",
    "for key in keys:\n",
    "    output_texts_concat = []\n",
    "\n",
    "    data = data_[key]\n",
    "    data = data.map(lambda x: {\n",
    "        \"test_prompt\": message_template(x['quiz']) \n",
    "    })\n",
    "    \n",
    "    # Generate completions for this batch\n",
    "    for i, data_batch in tqdm(enumerate(data.iter(batch_size=batch_size)), total=len(data)//batch_size):   \n",
    "        chat_inputs = tokenizer.apply_chat_template(data_batch[\"test_prompt\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=context_len, return_length=True, tokenize=False)\n",
    "        outputs = generate_batch(model, tokenizer, chat_inputs, max_new_tokens=context_len, temperature=temperature)\n",
    "        output_texts_concat.extend(outputs)\n",
    "\n",
    "    # Add completions column to dataset\n",
    "    column_name = f\"completions_{key}\"\n",
    "    data = data.add_column(column_name, output_texts_concat)\n",
    "    \n",
    "    # Evaluate completions\n",
    "    verified_column = f\"verified_{key}\"\n",
    "    discrepancies_column = f\"discrepancies_{key}\"\n",
    "    data = eval_dataset(data, column_name, verified_column, discrepancies_column)\n",
    "    \n",
    "    # Calculate score\n",
    "    score = data[verified_column].count(True) / len(data) * 100\n",
    "    print(f\"{key} score: {score:.2f}%\")\n",
    "    \n",
    "    # Store score and trajectories\n",
    "    results['scores'][key] = score\n",
    "    results['trajectories'][key] = []\n",
    "    \n",
    "    # Create trajectory data using the correct column names for each key\n",
    "    for i in range(len(data)):\n",
    "        results['trajectories'][key].append({\n",
    "            'completions': data[column_name][i],\n",
    "            'verified': data[verified_column][i],\n",
    "            'discrepancies': data[discrepancies_column][i]\n",
    "        })\n",
    "\n",
    "import json, os\n",
    "savepath = f\"./results/ood/{adapter}/knk.json\"\n",
    "os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "with open(savepath, 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verified_4ppl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"verified_{key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_texts_concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sos1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

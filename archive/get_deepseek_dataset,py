import time
import sys
import os
import concurrent.futures
import argparse
from typing import Optional, List, Dict, Any, Tuple
from openai import OpenAI
from huggingface_hub import HfApi
from tqdm import tqdm
import json
from datasets import load_dataset

class OnlineLM:
    """Online language model using API services."""
    
    def __init__(self, model_name: str, api_token: str, api_base_url: str, **kwargs):
        self.model = model_name  # Store model name as the model identifier
        self.temperature = kwargs.get("temperature", 0)  # Default temperature
        self.api_token = api_token
        self.api_base_url = api_base_url
        self._initialize()
        
    def _initialize(self):
        """Initialize the OpenAI client."""
        self.openai = OpenAI(
            api_key=self.api_token,
            base_url=self.api_base_url,
        )
    
    def _fetch_response(self, message_data):
        """Fetch a response from the API."""
        messages, max_tokens = message_data
        
        try:
            chat_completion = self.openai.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=self.temperature if self.temperature > 0 else 0     
            )
            reasoning_content = chat_completion.choices[0].message.reasoning_content
            content = chat_completion.choices[0].message.content
            
            return reasoning_content, content
        except Exception as e:
            return f"Error: {str(e)}", None
    
    def generate(self, 
                input_messages: List[Dict[str, Any]], 
                max_new_tokens: int = 100, 
                repeat_input: bool = False) -> Tuple[List[Any], Any]:
        """Generate text using the API."""
        
        # Prepare batch of requests
        request_data = []
        for messages in input_messages:
            request_data.append((messages, max_new_tokens))
        
        # Process in parallel
        results = []
        with concurrent.futures.ThreadPoolExecutor() as executor:
            api_results = list(executor.map(self._fetch_response, request_data))
            
            for i, content in enumerate(api_results):
                if repeat_input:
                    # Append generation to the original message
                    new_message = input_messages[i].copy()
                    new_message[-1] = new_message[-1].copy()
                    new_message[-1]['content'] += content
                    results.append(new_message)
                else:
                    results.append(content)
        
        # For API compatibility, return both results and a metadata object
        metadata = {"model": self.model, "online": True}
        time.sleep(0.02)
        return results, metadata

def main():
    parser = argparse.ArgumentParser(description='Process dataset with OnlineLM')
    parser.add_argument('--api_token', required=True, help='API token for the service')
    parser.add_argument('--mod_value', type=int, required=True, help='Process only indices where i %% 3 == mod_value')
    parser.add_argument('--output_file', required=True, help='Output file path')
    parser.add_argument('--batch_size', type=int, default=50, help='Processing batch size')
    parser.add_argument('--max_tokens', type=int, default=256, help='Maximum tokens for generation')
    parser.add_argument('--api_base_url', default="https://api.deepseek.com/v1", help='Base URL for API')
    parser.add_argument('--model', default="deepseek-reasoner", help='Model name')
    parser.add_argument('--temperature', type=float, default=0, help='Temperature for generation')
    
    args = parser.parse_args()
    
    # Initialize the model
    model = OnlineLM(args.model, args.api_token, args.api_base_url, temperature=args.temperature)
    api = HfApi()

    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)

    # Load the dataset
    print("Loading dataset from Hugging Face...")
    data = load_dataset("MelinaLaimon/stream-of-search", split="train[90%:100%]")

    # Create a copy of the original dataset to work with - convert to list for easier manipulation
    working_data = data.to_list()

    # Prepare user messages and identify entries needing processing
    needs_processing_indices = []

    for i, example in enumerate(working_data):
        if example.get('messages_deepseek') is None or len(example.get('messages_deepseek')) < 2:
            # Create deepseek messages
            example['messages_deepseek'] = [
                {
                    'role': message['role'],
                    'content': message['content'] + "\nNote that the solution must exist" if message['role'] == "user" else message['content']
                }
                for message in example["messages_sos"] if 'role' in message and message["role"] == "user"
            ]
            needs_processing_indices.append(i)

    # Filter by mod value
    needs_processing_indices = [i for i in needs_processing_indices if i % 3 == args.mod_value]

    if not needs_processing_indices:
        print(f"No entries found where messages_deepseek is None and index %% 3 == {args.mod_value}. Nothing to process.")
    else:
        print(f"Found {len(needs_processing_indices)} entries that need processing.")
        
        # Process in batches
        for batch_start in tqdm(range(0, len(needs_processing_indices), args.batch_size)):
            batch_indices = needs_processing_indices[batch_start:batch_start + args.batch_size]
            if not batch_indices:
                continue
            
            # Extract the batch that needs processing
            data_batch = [working_data[i] for i in batch_indices]
            deepseek_messages = [item['messages_deepseek'] for item in data_batch]
                    
            # Generate responses for this batch
            results, metadata = model.generate(deepseek_messages, max_new_tokens=args.max_tokens)
            
            # Update the working dataset with the new messages
            for j, original_idx in enumerate(batch_indices):
                if j < len(results):  # Safety check
                    reasoning_content, content = results[j]
                    
                    # Check if there was an error or if content is None
                    if content is None or reasoning_content.startswith("Error:"):
                        # If there's an error, keep messages_deepseek as None
                        working_data[original_idx]["messages_deepseek"] = None
                        print(f"Error processing item {original_idx}: {reasoning_content}")
                    else:
                        # No error, update with the generated content
                        user_prompt = working_data[original_idx]["messages_sos"][0]["content"]
                        search_path = f"{reasoning_content}\n</think>\n{content}"
                        
                        # Update the messages_deepseek field
                        working_data[original_idx]["messages_deepseek"] = [
                            {"role": "user", "content": user_prompt},
                            {"role": "assistant", "content": search_path}
                        ]
            
            # Save the updated dataset to a file after each batch
            with open(args.output_file, "w") as f:
                json.dump(working_data, f, indent=4)

if __name__ == "__main__":
    main()
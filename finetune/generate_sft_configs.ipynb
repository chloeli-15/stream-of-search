{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated config file: ./recipes/qwen-2.5/sft/config_deepseek_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_sos_react_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_sos_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_optimal_1k.yaml\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "train_dataset_size = {\n",
    "    # \"0_5k\": 0.05, \n",
    "    # \"1k\": 0.1, \n",
    "    # \"5k\": 0.5, \n",
    "    # \"10k\": 1.0\n",
    "    \"1k\": 1.0,\n",
    "}\n",
    "\n",
    "messages_field_keys = {\n",
    "    \"deepseek\": \"messages\", \n",
    "    \"sos_react\": \"messages\", \n",
    "    \"sos\": \"messages\", \n",
    "    \"optimal\": \"messages\"\n",
    "    # \"optimal\": \"messages_optimal\",\n",
    "    # \"search\": \"messages_sos\",\n",
    "    # \"search-react\": \"messages_sos_react\",\n",
    "    # \"deepseek_r1_distill_llama_70b\": \"messages_deepseek_r1_distill_llama_70b\",\n",
    "    # \"deepseek\": \"messages_deepseek\",\n",
    "}\n",
    "\n",
    "def generate_config(base_config_path: str, dataset_size: str, traj_type: str, dataset_name: str = \"yeok/stream-of-search-dataset_deepseek\"):\n",
    "    # Read the original file to extract content\n",
    "    with open(base_config_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse the YAML content\n",
    "    # Split by the comment section to separate modifiable fields\n",
    "    sections = re.split(r\"# These fields will be modified by the script\", content, 1)\n",
    "    \n",
    "    if len(sections) > 1:\n",
    "        first_part = sections[0]\n",
    "        second_part = sections[1]\n",
    "        \n",
    "        # Extract chat template section\n",
    "        chat_template_match = re.search(r\"chat_template:\\s*\\|[\\s\\S]+?(?=\\n# These fields|$)\", content)\n",
    "        chat_template = chat_template_match.group(0) if chat_template_match else \"\"\n",
    "        \n",
    "        # Remove chat template from first part if it exists there\n",
    "        if chat_template:\n",
    "            first_part = first_part.replace(chat_template, \"\").strip()\n",
    "        \n",
    "        # Load the first part as YAML (the non-modifiable part)\n",
    "        config1 = yaml.safe_load(first_part) or {}\n",
    "        \n",
    "        # Load modifiable fields from second part as YAML \n",
    "        # but we'll only use this to get fields we're not explicitly modifying\n",
    "        config2_text = second_part.split(\"# SFT trainer config\", 1)\n",
    "        modifiable_part = config2_text[0] if len(config2_text) > 0 else \"\"\n",
    "        config2 = yaml.safe_load(modifiable_part) or {}\n",
    "        \n",
    "        # Load the trainer config part\n",
    "        trainer_config_part = \"# SFT trainer config\" + config2_text[1] if len(config2_text) > 1 else \"\"\n",
    "        trainer_config = yaml.safe_load(trainer_config_part.replace(\"# SFT trainer config\", \"\")) or {}\n",
    "        \n",
    "        # Generate model name and related paths\n",
    "        model_name = f\"qwen-2.5-1.5B-instruct-sft-lora-countdown-{traj_type}-{dataset_size}\"\n",
    "        \n",
    "        # Create the set of modifiable fields we want to update\n",
    "        updated_modifiable_fields = {\n",
    "            \"dataset_mixer\": {dataset_name: float(train_dataset_size[dataset_size])},\n",
    "            \"dataset_splits\": config2.get(\"dataset_splits\", [\"train\", \"test\"]),\n",
    "            \"preprocessing_num_workers\": config2.get(\"preprocessing_num_workers\", 12),\n",
    "            \"dataset_message_key\": messages_field_keys[traj_type],\n",
    "        }\n",
    "        \n",
    "        # Update trainer config fields\n",
    "        updated_trainer_config = trainer_config.copy()\n",
    "        updated_trainer_config[\"hub_model_id\"] = model_name\n",
    "        updated_trainer_config[\"output_dir\"] = f\"./models/{model_name}\"\n",
    "        updated_trainer_config[\"logging_dir\"] = f\"./logs/{model_name}\"\n",
    "        \n",
    "        # Create output directory path\n",
    "        output_dir = os.path.dirname(os.path.dirname(base_config_path))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_file = f\"{output_dir}/config_{traj_type}_{dataset_size}.yaml\"\n",
    "        \n",
    "        # Write the combined content to the output file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            # Write the first part (non-modifiable fields)\n",
    "            f.write(yaml.dump(config1, default_flow_style=False, sort_keys=False).strip() + \"\\n\")\n",
    "            \n",
    "            # Add chat template if it was before the modifiable fields\n",
    "            if chat_template_match and content.find(chat_template) < content.find(\"# These fields will be modified by the script\"):\n",
    "                f.write(chat_template + \"\\n\\n\")\n",
    "            \n",
    "            # Write the comment for modifiable fields\n",
    "            f.write(\"# These fields will be modified by the script\\n\")\n",
    "            \n",
    "            # Write the updated modifiable fields\n",
    "            f.write(yaml.dump(updated_modifiable_fields, default_flow_style=False, sort_keys=False))\n",
    "            \n",
    "            # Write SFT trainer config\n",
    "            f.write(\"\\n# SFT trainer config\\n\")\n",
    "            f.write(yaml.dump(updated_trainer_config, default_flow_style=False, sort_keys=False))\n",
    "            \n",
    "            # Add chat template if it was after the modifiable fields\n",
    "            if chat_template_match and content.find(chat_template) > content.find(\"# These fields will be modified by the script\"):\n",
    "                f.write(\"\\n\" + chat_template)\n",
    "    \n",
    "    else:\n",
    "        # If the file doesn't have the expected structure, just do a simple update\n",
    "        config = yaml.safe_load(content) or {}\n",
    "        \n",
    "        # Generate model name and related paths\n",
    "        model_name = f\"qwen-2.5-1.5B-instruct-sft-lora-countdown-{traj_type}-{dataset_size}\"\n",
    "        \n",
    "        # Update config values\n",
    "        config[\"dataset_mixer\"] = {dataset_name: float(train_dataset_size[dataset_size])}\n",
    "        config[\"hub_model_id\"] = model_name\n",
    "        config[\"output_dir\"] = f\"./models/{model_name}\"\n",
    "        config[\"logging_dir\"] = f\"./logs/{model_name}\"\n",
    "        config[\"dataset_message_key\"] = messages_field_keys[traj_type]\n",
    "        \n",
    "        # Extract chat template\n",
    "        chat_template_match = re.search(r\"chat_template:\\s*\\|[\\s\\S]+\", content)\n",
    "        chat_template = chat_template_match.group(0) if chat_template_match else \"\"\n",
    "        \n",
    "        # Remove chat template from config\n",
    "        if \"chat_template\" in config:\n",
    "            del config[\"chat_template\"]\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = os.path.dirname(os.path.dirname(base_config_path))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_file = f\"{output_dir}/config_{traj_type}_{dataset_size}.yaml\"\n",
    "        \n",
    "        # Write the config\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(yaml.dump(config, default_flow_style=False, sort_keys=False))\n",
    "            if chat_template:\n",
    "                f.write(\"\\n\" + chat_template)\n",
    "    \n",
    "    print(f\"Generated config file: {output_file}\")\n",
    "\n",
    "for dataset_size in train_dataset_size.keys():\n",
    "    for traj_type in messages_field_keys.keys():\n",
    "        generate_config(\n",
    "            base_config_path=\"./recipes/qwen-2.5/sft/base_configs/config_lora.yaml\",\n",
    "            dataset_size=dataset_size, \n",
    "            traj_type=traj_type\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sos1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated config file: ./recipes/qwen-2.5/sft/config_optimal_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_search_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_search-react_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_deepseek_r1_distill_llama_70b_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_deepseek_1k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_optimal_5k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_search_5k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_search-react_5k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_deepseek_r1_distill_llama_70b_5k.yaml\n",
      "Generated config file: ./recipes/qwen-2.5/sft/config_deepseek_5k.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "train_dataset_size = {\n",
    "    \"1k\": 0.1, \n",
    "    \"5k\": 0.5, \n",
    "    # \"10k\": 1.0\n",
    "}\n",
    "\n",
    "messages_field_keys = {\n",
    "    \"optimal\": \"messages_optimal\",\n",
    "    \"search\": \"messages_sos\",\n",
    "    \"search-react\": \"messages_sos_react\",\n",
    "    \"deepseek_r1_distill_llama_70b\": \"messages_deepseek_r1_distill_llama_70b\",\n",
    "    \"deepseek\": \"messages_deepseek\",\n",
    "}\n",
    "\n",
    "def generate_config(base_config_path: str, dataset_size: str, traj_type: str, dataset_name: str = \"MelinaLaimon/stream-of-search\"):\n",
    "    # Read the original file to extract content\n",
    "    with open(base_config_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse the YAML content\n",
    "    # Split by the comment section to separate modifiable fields\n",
    "    sections = re.split(r\"# These fields will be modified by the script\", content, 1)\n",
    "    \n",
    "    if len(sections) > 1:\n",
    "        first_part = sections[0]\n",
    "        second_part = sections[1]\n",
    "        \n",
    "        # Extract chat template section\n",
    "        chat_template_match = re.search(r\"chat_template:\\s*\\|[\\s\\S]+?(?=\\n# These fields|$)\", content)\n",
    "        chat_template = chat_template_match.group(0) if chat_template_match else \"\"\n",
    "        \n",
    "        # Remove chat template from first part if it exists there\n",
    "        if chat_template:\n",
    "            first_part = first_part.replace(chat_template, \"\").strip()\n",
    "        \n",
    "        # Load the first part as YAML (the non-modifiable part)\n",
    "        config1 = yaml.safe_load(first_part) or {}\n",
    "        \n",
    "        # Load modifiable fields from second part as YAML \n",
    "        # but we'll only use this to get fields we're not explicitly modifying\n",
    "        config2_text = second_part.split(\"# SFT trainer config\", 1)\n",
    "        modifiable_part = config2_text[0] if len(config2_text) > 0 else \"\"\n",
    "        config2 = yaml.safe_load(modifiable_part) or {}\n",
    "        \n",
    "        # Load the trainer config part\n",
    "        trainer_config_part = \"# SFT trainer config\" + config2_text[1] if len(config2_text) > 1 else \"\"\n",
    "        trainer_config = yaml.safe_load(trainer_config_part.replace(\"# SFT trainer config\", \"\")) or {}\n",
    "        \n",
    "        # Generate model name and related paths\n",
    "        model_name = f\"qwen-2.5-1.5B-instruct-sft-lora-countdown-{traj_type}-{dataset_size}\"\n",
    "        \n",
    "        # Create the set of modifiable fields we want to update\n",
    "        updated_modifiable_fields = {\n",
    "            \"dataset_mixer\": {dataset_name: float(train_dataset_size[dataset_size])},\n",
    "            \"dataset_splits\": config2.get(\"dataset_splits\", [\"train\", \"test\"]),\n",
    "            \"preprocessing_num_workers\": config2.get(\"preprocessing_num_workers\", 12),\n",
    "            \"dataset_message_key\": messages_field_keys[traj_type],\n",
    "        }\n",
    "        \n",
    "        # Update trainer config fields\n",
    "        updated_trainer_config = trainer_config.copy()\n",
    "        updated_trainer_config[\"hub_model_id\"] = model_name\n",
    "        updated_trainer_config[\"output_dir\"] = f\"./models/{model_name}\"\n",
    "        updated_trainer_config[\"logging_dir\"] = f\"./logs/{model_name}\"\n",
    "        \n",
    "        # Create output directory path\n",
    "        output_dir = os.path.dirname(os.path.dirname(base_config_path))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_file = f\"{output_dir}/config_{traj_type}_{dataset_size}.yaml\"\n",
    "        \n",
    "        # Write the combined content to the output file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            # Write the first part (non-modifiable fields)\n",
    "            f.write(yaml.dump(config1, default_flow_style=False, sort_keys=False).strip() + \"\\n\")\n",
    "            \n",
    "            # Add chat template if it was before the modifiable fields\n",
    "            if chat_template_match and content.find(chat_template) < content.find(\"# These fields will be modified by the script\"):\n",
    "                f.write(chat_template + \"\\n\\n\")\n",
    "            \n",
    "            # Write the comment for modifiable fields\n",
    "            f.write(\"# These fields will be modified by the script\\n\")\n",
    "            \n",
    "            # Write the updated modifiable fields\n",
    "            f.write(yaml.dump(updated_modifiable_fields, default_flow_style=False, sort_keys=False))\n",
    "            \n",
    "            # Write SFT trainer config\n",
    "            f.write(\"\\n# SFT trainer config\\n\")\n",
    "            f.write(yaml.dump(updated_trainer_config, default_flow_style=False, sort_keys=False))\n",
    "            \n",
    "            # Add chat template if it was after the modifiable fields\n",
    "            if chat_template_match and content.find(chat_template) > content.find(\"# These fields will be modified by the script\"):\n",
    "                f.write(\"\\n\" + chat_template)\n",
    "    \n",
    "    else:\n",
    "        # If the file doesn't have the expected structure, just do a simple update\n",
    "        config = yaml.safe_load(content) or {}\n",
    "        \n",
    "        # Generate model name and related paths\n",
    "        model_name = f\"qwen-2.5-1.5B-instruct-sft-lora-countdown-{traj_type}-{dataset_size}\"\n",
    "        \n",
    "        # Update config values\n",
    "        config[\"dataset_mixer\"] = {dataset_name: float(train_dataset_size[dataset_size])}\n",
    "        config[\"hub_model_id\"] = model_name\n",
    "        config[\"output_dir\"] = f\"./models/{model_name}\"\n",
    "        config[\"logging_dir\"] = f\"./logs/{model_name}\"\n",
    "        config[\"dataset_message_key\"] = messages_field_keys[traj_type]\n",
    "        \n",
    "        # Extract chat template\n",
    "        chat_template_match = re.search(r\"chat_template:\\s*\\|[\\s\\S]+\", content)\n",
    "        chat_template = chat_template_match.group(0) if chat_template_match else \"\"\n",
    "        \n",
    "        # Remove chat template from config\n",
    "        if \"chat_template\" in config:\n",
    "            del config[\"chat_template\"]\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = os.path.dirname(os.path.dirname(base_config_path))\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_file = f\"{output_dir}/config_{traj_type}_{dataset_size}.yaml\"\n",
    "        \n",
    "        # Write the config\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(yaml.dump(config, default_flow_style=False, sort_keys=False))\n",
    "            if chat_template:\n",
    "                f.write(\"\\n\" + chat_template)\n",
    "    \n",
    "    print(f\"Generated config file: {output_file}\")\n",
    "\n",
    "for dataset_size in train_dataset_size.keys():\n",
    "    for traj_type in messages_field_keys.keys():\n",
    "        generate_config(\n",
    "            base_config_path=\"./recipes/qwen-2.5/sft/base_configs/config_lora.yaml\",\n",
    "            dataset_size=dataset_size, \n",
    "            traj_type=traj_type\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts/run_sft.py messages_sos_react\n",
    "# max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['nums', 'target', 'solution', 'rating', 'search_type', 'heuristic', 'messages_optimal', 'messages_sos_react', 'messages_sos', 'messages_deepseek_r1_distill_llama_70b', 'messages_deepseek'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "data_all = datasets.load_dataset(\"MelinaLaimon/stream-of-search\")\n",
    "data = data_all[\"train\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to verify a generated config file\n",
    "def check_config_file(config_path):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Check if any placeholders remain in the file\n",
    "    remaining_placeholders = re.findall(r'\\{\\{\\s*(.*?)\\s*\\}\\}', content)\n",
    "    if remaining_placeholders:\n",
    "        print(f\"Error: Found unreplaced placeholders: {remaining_placeholders}\")\n",
    "    else:\n",
    "        print(\"Success: All placeholders were replaced.\")\n",
    "    \n",
    "    # Try to parse as YAML to ensure it's valid\n",
    "    try:\n",
    "        config_data = yaml.safe_load(content)\n",
    "        print(\"Success: Config file is valid YAML.\")\n",
    "        # Check if dataset_mixer was properly formatted\n",
    "        if 'dataset_mixer' in config_data:\n",
    "            print(f\"Dataset mixer: {config_data['dataset_mixer']}\")\n",
    "        else:\n",
    "            print(\"Warning: 'dataset_mixer' key not found in config\")\n",
    "            print(f\"Available keys: {list(config_data.keys() if config_data else [])}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"Error: Config file is not valid YAML: {e}\")\n",
    "\n",
    "# Run this to check the original template file structure\n",
    "try:\n",
    "    check_config_file(\"./recipes/qwen-2.5/sft/config_lora.yaml\")\n",
    "    print(\"\\nExamining original template file:\")\n",
    "    with open(\"./recipes/qwen-2.5/sft/config_lora.yaml\", \"r\") as f:\n",
    "        template = f.read()\n",
    "        print(template[:1000] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading template file: {e}\")\n",
    "\n",
    "# Check a generated file (run after generating)\n",
    "# check_config_file(\"./recipes/qwen-2.5/sft/config_optimal_1k.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this cell to inspect file contents directly\n",
    "def inspect_file_for_placeholders(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"File size: {len(content)} bytes\")\n",
    "        \n",
    "        # Look for special patterns that might indicate placeholder format issues\n",
    "        print(\"\\nSearching for placeholder patterns:\")\n",
    "        patterns = [r'\\{\\{\\s*(.*?)\\s*\\}\\}', r'\\{[^{}]+\\}', r'\\$\\{.*?\\}']\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, content)\n",
    "            if matches:\n",
    "                print(f\"Pattern '{pattern}' matches: {matches}\")\n",
    "        \n",
    "        # Print lines containing potential placeholders for context\n",
    "        print(\"\\nLines containing potential placeholders:\")\n",
    "        lines = content.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if '{{' in line or '}}' in line or '{%' in line or '%}' in line:\n",
    "                print(f\"Line {i+1}: {line}\")\n",
    "        \n",
    "        # Try different encoding checks\n",
    "        print(\"\\nChecking for encoding issues:\")\n",
    "        unusual_chars = [char for char in content if ord(char) > 127]\n",
    "        if unusual_chars:\n",
    "            print(f\"Found unusual characters: {set(unusual_chars)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting file: {e}\")\n",
    "\n",
    "# Run this to inspect the template file\n",
    "inspect_file_for_placeholders(\"./recipes/qwen-2.5/sft/config_lora.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sos1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
